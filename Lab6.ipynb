{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresja liniowa z regularyzacją\n",
    "\n",
    "### Czytanka\n",
    "Fajne tutoriale, dostarczające sporo intuicji:\n",
    "* https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n",
    "* https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "* Obszerny przykład, omawiający charakterystykę obydwu metod regularyzacji i wpływ doboru parametrów na sposób dopasowania: https://github.com/justmarkham/DAT4/blob/master/notebooks/08_linear_regression.ipynb\n",
    "* Konkurs na Kaggle, który dostarcza przydatnego zbioru danych: https://www.kaggle.com/apapiu/regularized-linear-models\n",
    "* Opis implementacji: http://www.geeksforgeeks.org/linear-regression-python-implementation/\n",
    "* (*) Implementacja prostej regresji liniowej (od zera): http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/\n",
    "* (\\**) Implementacja regresji dla wielu zmiennych: http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/\n",
    "* Gradient descent: https://en.wikipedia.org/wiki/Gradient_descent\n",
    "\n",
    "### Podstawowe info\n",
    "#### 1. Regresja w wielu wymiarach:\n",
    "Bardzo często chcemy dopasować prostą do danych o większej wymiarowości. Przykładowo, cena domu może zależeć od jego powierzchni, ilości pokoi i ilości pięter (w praktyce takich cech danych może być bardzo dużo). Na szczęście idea jest w zasadzie identyczna, jak przy jednowymiarowej regresji z poprzednich zajęć. Zwróćmy uwagę, że gdy umiemy dopasowywać funkcję liniową w wielu wymiarach, możemy robić różne zagraniczne tricki: nikt nie broni nam sztucznie stworzyć sobie nowych zmiennych, będących nieliniowymi przekształceniami istniejących zmiennych. W ten sposób za pomocą jednego algorytmu możemy dopasowywać zarówno funkcje liniowe, jak i np. wielomiany. (Więcej: na zajęciach). <img src=\"images/dataset.jpg\">\n",
    "\n",
    "Swoją drogą, w internecie wymiennie można znaleźć pojęcia \"multi(ple) regression\" i \"multivariate regression\". Można założyć, że pierwsza z nich odnosi się do funkcji z wielowymiarową dziedziną, ale jednowymiarową przeciwdziedziną, natomiast to drugie: z wielowymiarową dziedziną i przeciwdziedziną.\n",
    "\n",
    "#### 2. **Gradient descent**:\n",
    "O ile dla małych/prostych zbiorów danych jesteśmy w stanie wyliczyć najlepsze dopasowanie analitycznie (por. MOwNiT 1, laboratorium), o tyle w praktyce zajęłoby to zbyt dużo czasu. Na szczęście minimalizować funkcję błędu możemy heurystycznie, za pomocą poruszania się w kierunku największego spadku. Intuicja: to tak, jakbyśmy chcieli znaleźć najniższe miejsce w jakiejś kotlinie poprzez rzucenie na ziemię metalowej kulki i sprawdzenie, gdzie się zatrzyma.\n",
    "<img src=\"images/gradient_descent.png\">\n",
    "Tak prosta metoda może dość łatwo \"utknąć\" w minimum lokalnym, ale na szczęście funkcje, z którymi mamy do czynienia przy regresji liniowej są bardzo porządne i mają tylko jedno minimum.\n",
    "\n",
    "#### 3. **Regularyzacja**:\n",
    "Intuicja podpowiada \"im więcej cech danych, tym lepiej je zrozumiemy\". Niestety, życie (i konkursy na Kaggle) uczy, że to błędna intuicja. Część zmiennych nie wnosi żadnej informacji, dane są zaszumione, a czasem informacja jest redundantna. Co się dzieje, jeśli dwie cechy są mocno skorelowane? Jeśli są również mocno skorelowane z wartością, którą przewidujemy, zaczniemy tak naprawdę liczyć tą samą cechę, ale podwójnie (to intuicja, nie poprawne tłumaczenie).\n",
    "\n",
    "\n",
    "### Pytania\n",
    "* w jakich sytuacjach zwykła regresja daje złe rezultaty?\n",
    "* dlaczego chcemy \"karać\" model za wysokie wagi?\n",
    "\n",
    "### Zadania\n",
    "1. Napisać regresję dla wielu wymiarów.\n",
    "2. Zaimplementować regularyzację L1 w powyższej funkcji\n",
    "3. Zaimplementować regularyzację L2\n",
    "4. Rozszerzyć klasę `LinearRegressor` z poprzednich zajęć tak, by używała nowej, wielowymiarowej regresji i pozwalała przekazać opcjonalny parametr `regularization`, o wartościach `l1` lub `l2`, i na jego podstawie dokonująca odpowiedniego rodzaju regularyzacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def multiregression(X: np.matrix, y: np.matrix) -> np.matrix:\n",
    "    \"\"\"Calculate multiple regression for a feature matrix `X` and a vector `y`.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla przypomnienia, klasa, o której mowa w zadaniu 4. Co trzeba w niej zmienić?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class LinearRegressor():\n",
    "    def __init__(self):\n",
    "        self._coeffs = None   # type: Optional[Tuple[float, float]]\n",
    "    \n",
    "    def fit(self, x: List[float], y: List[float]) -> None:\n",
    "        pass\n",
    "\n",
    "    def predict(self, x: List[float]) -> List[float]:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def coeffs(self) -> Tuple[float, float]:\n",
    "        if self._coeffs is None:\n",
    "            raise Exception('You need to call `fit` on the model first.')\n",
    "        \n",
    "        return self._coeffs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
